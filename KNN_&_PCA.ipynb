{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziO0Ne1Bksc0"
      },
      "outputs": [],
      "source": [
        "1.  : What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "     -> K-Nearest Neighbors (KNN) is a supervised, non-parametric algorithm that makes predictions by finding the k\n",
        "       closest data points in a training set to a new data point and using their information.\n",
        "\n",
        "\n",
        "# How it Works\n",
        "\n",
        "# Store Training Data:\n",
        "           The algorithm stores the entire training dataset.\n",
        "# Choose 'k':\n",
        "           A user specifies the number of neighbors, k, to consider.\n",
        "# Calculate Distance:\n",
        "             For a new data point, the algorithm calculates the distance to all other data points in the\n",
        "               training set, often using Euclidean distance.\n",
        "# Find k Nearest Neighbors:\n",
        "                The algorithm identifies the k training data points that are closest to the new point.\n",
        "# Make a Prediction:\n",
        "*  Classification:\n",
        "             The new data point is assigned the class that appears most often among the k nearest\n",
        "                 neighbors (a \"majority vote\").\n",
        "*   Regression:\n",
        "            The algorithm calculates the average (or a weighted average) of the target variable values of\n",
        "               the k nearest neighbors to predict a continuous value.\n",
        "\n",
        "\n",
        "\n",
        "# Key Characteristics\n",
        "\n",
        "# Supervised Learning:\n",
        "             KNN requires labeled training data.\n",
        "# Non-Parametric:\n",
        "             It makes no assumptions about the underlying data distribution, making it flexible for complex, non-linear data.\n",
        "# Lazy Learning:\n",
        "         KNN is a \"lazy\" or instance-based learning algorithm because it doesn't build an explicit model during training;\n",
        "    *   it stores the data and performs computations only when a prediction is requested"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.  : What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "    -> The Curse of Dimensionality refers to how machine learning algorithms can struggle in high-dimensional spaces because\n",
        "         data points become increasingly sparse, making it difficult to find meaningful neighbors.\n",
        "\n",
        "        # Curse of Dimensionality:-\n",
        "\n",
        "# High-dimensional data:\n",
        "            This refers to datasets with a large number of features or attributes.\n",
        "# Data Sparsity:\n",
        "          As the number of dimensions grows, the volume of the feature space increases exponentially, causing\n",
        "            the data points to become more spread out and sparse.\n",
        "# Difficulty in pattern recognition:\n",
        "           In sparse, high-dimensional spaces, it becomes harder to identify patterns and relationships within the data.\n",
        "# Increased Data Requirements:\n",
        "           To maintain a similar density as in lower dimensions, the amount of training data needed grows exponentially\n",
        "             with the number of dimensions.\n",
        "\n",
        "\n",
        "      #  How it affects KNN Performance\n",
        "1. Loss of Meaningful \"Closeness\":\n",
        "                KNN relies on finding the nearest neighbors to a given data point. In high-dimensional spaces, the concept of\n",
        "                closeness is diminished because all points tend to be far from each other.\n",
        "2. Diluted Distance Metrics:\n",
        "             The distance between points becomes less discriminative, meaning that most points are roughly the same distance\n",
        "               away from a target point.\n",
        "3. Increased Noise:\n",
        "            With more irrelevant features added to the dataset, the noise in the data increases, making it harder for the algorithm\n",
        "               to distinguish true neighbors from noise.\n"
      ],
      "metadata": {
        "id": "ERtWhMZTl8jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.  : What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "    -> Principal Component Analysis (PCA) is a feature extraction technique that creates new, uncorrelated features (principal components)\n",
        "     from existing ones to reduce data dimensionality while preserving most of the original information.\n",
        "\n",
        "\n",
        "\n",
        "    #  Principal Component Analysis (PCA)\n",
        "\n",
        "# What it does:\n",
        "       PCA is an unsupervised learning method for feature extraction.\n",
        "# How it works:\n",
        "      It transforms a dataset into a new set of uncorrelated variables called principal components, which are linear\n",
        "        combinations of the original features.\n",
        "# Purpose:\n",
        "       To reduce the dimensionality of a dataset by identifying the directions (principal components) in the data that capture\n",
        "         the most variance. This helps to simplify models, reduce multicollinearity, and potentially speed up training.\n",
        "\n",
        "\n",
        "        #  Feature Selection\n",
        "\n",
        "# What it is:\n",
        "       A process that reduces the number of features by selecting a subset of the most relevant original features.\n",
        "# How it works:\n",
        "      It ranks and selects variables based on their importance for predicting a target variable or their ability to describe the data's variance.\n",
        "# Purpose:\n",
        "      To improve model interpretability, reduce training time, decrease the chance of overfitting, and remove irrelevant or noisy features.\n",
        "\n",
        "\n",
        "      # Key Differences\n",
        "# Feature Creation vs. Subset Selection:\n",
        "        PCA creates new features (principal components), while feature selection selects a subset of existing features.\n",
        "# Interpretability:\n",
        "          Feature selection provides high interpretability because it uses the original features. PCA sacrifices interpretability\n",
        "            for dimensionality reduction, as its components are linear combinations of the originals.\n",
        "# Goal:\n",
        "     While both aim to reduce dimensionality, PCA focuses on transforming features to capture maximum variance with fewer dimensions,\n",
        "       whereas feature selection focuses on identifying and keeping the most informative original features.\n"
      ],
      "metadata": {
        "id": "oBI2a0lBl8lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.  : What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "     ->  In PCA, eigenvectors define the directions of maximum variance in the data, forming new axes called principal components.\n",
        "          The eigenvalues are scalar values associated with these eigenvectors, indicating the amount of variance (or information)\n",
        "            captured along each principal component.\n",
        "\n",
        "\n",
        "    # Eigenvectors in PCA\n",
        "# Directions of Variance:\n",
        "       Eigenvectors represent the fundamental directions of data variation.\n",
        "# Principal Components:\n",
        "        These eigenvectors are orthogonal (perpendicular) to each other and are referred to as the principal components.\n",
        "# New Axes:\n",
        "      They form a new coordinate system for the data, where the first principal component is the direction of greatest\n",
        "        variance, the second is the next greatest, and so on.\n",
        "\n",
        "\n",
        "        # Why They Are Important\n",
        "\n",
        "# 1. Dimensionality Reduction:\n",
        "         By ordering the eigenvectors by their corresponding eigenvalues, PCA can identify the principal components that explain\n",
        "         the most variance. Choosing the top eigenvectors (e.g., the first few with the largest eigenvalues) allows for reducing the\n",
        "          dimensionality of the dataset while retaining most of the original information.\n",
        "# 2. Feature Extraction:\n",
        "         Eigenvectors and eigenvalues help in transforming complex, high-dimensional data into a lower-dimensional representation by\n",
        "           finding the most informative new features (principal components).\n",
        "# 3. Data Simplification:\n",
        "          They provide a way to simplify datasets by focusing on the most significant patterns of variation, which can improve the\n",
        "           performance of machine learning algorithms by reducing noise and multicollinearity.\n",
        "# 4. Visualization:\n",
        "        They facilitate the visualization of high-dimensional data by projecting it onto a lower-dimensional space (typically 2D or 3D)\n",
        "          defined by the most significant principal components.\n",
        "\n"
      ],
      "metadata": {
        "id": "PKAfSKy4n6lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.   How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "    #  -> How the Pipeline Works\n",
        "\n",
        "# 1. PCA for Dimensionality Reduction:\n",
        "          *  The pipeline begins with Principal Component Analysis (PCA) applied to the high-dimensional dataset.\n",
        "          *  PCA identifies principal components, which are linear combinations of the original features that capture the most variance in the data.\n",
        "          *  It transforms the data into a new, lower-dimensional space by keeping only the most significant principal components.\n",
        "# 2. KNN for Classification:\n",
        "         The transformed, lower-dimensional data is then used as input for the KNN algorithm.\n",
        "          KNN classifies a new data point by finding its nearest neighbors in the reduced feature space and assigning the majority class among them.\n",
        "\n",
        "\n",
        "          # Why This Combination is Effective\n",
        "\n",
        "# Combats the Curse of Dimensionality:\n",
        "           High-dimensional datasets can make KNN less effective due to increased sparsity and noise, a phenomenon known as the \"curse of dimensionality\".\n",
        "                 PCA helps mitigate this by reducing the number of dimensions, making the data more manageable for KNN's distance calculations.\n",
        "# Reduces Computational Cost:\n",
        "          By reducing the number of features, PCA decreases the computational complexity and time required for KNN to calculate distances\n",
        "            between data points, leading to faster predictions.\n",
        "# Removes Noise and Redundancy:\n",
        "            PCA focuses on capturing the most informative components, effectively filtering out less relevant or redundant information from\n",
        "              the original features. This results in a cleaner dataset for KNN, potentially improving classification accuracy.\n"
      ],
      "metadata": {
        "id": "S9FDZ4wjl8oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.  Train a KNN Classifier on the Wine dataset with and without feature scaling.\n",
        " Compare model accuracy in both cases.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---- KNN WITHOUT FEATURE SCALING ----\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---- KNN WITH FEATURE SCALING ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy WITHOUT scaling: {:.2f}\".format(accuracy_no_scaling))\n",
        "print(\"Accuracy WITH scaling   : {:.2f}\".format(accuracy_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHD3Jxyel8rI",
        "outputId": "0f956485-5d6e-4c94-edbf-972d93219001"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.72\n",
            "Accuracy WITH scaling   : 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7.   : Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Scale features before PCA (important for variance-based methods)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"PC{i}: {ratio:.4f}\")\n",
        "\n",
        "# Print cumulative variance\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "print(np.cumsum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtntBNISl80V",
        "outputId": "8ed67d9d-5454-4f1f-b046-4adbecbfd551"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Cumulative explained variance:\n",
            "[0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
            " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8. : Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components).\n",
        "     Compare the accuracy with the original dataset.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features (important for PCA & KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---- KNN on ORIGINAL dataset ----\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ---- PCA Transformation (retain top 2 components) ----\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# KNN on PCA-reduced dataset\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on ORIGINAL dataset: {:.2f}\".format(acc_original))\n",
        "print(\"Accuracy on PCA (2 components): {:.2f}\".format(acc_pca))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeM2W9ltl84j",
        "outputId": "f6c8670e-f2fd-4200-cb65-c65d5abfec2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on ORIGINAL dataset: 0.94\n",
            "Accuracy on PCA (2 components): 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9.  : Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------- KNN with Euclidean distance -------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# ------------------- KNN with Manhattan distance -------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# ------------------- Results -------------------\n",
        "print(\"Accuracy with Euclidean distance : {:.2f}\".format(acc_euclidean))\n",
        "print(\"Accuracy with Manhattan distance : {:.2f}\".format(acc_manhattan))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz1lckAel87g",
        "outputId": "3d6d71db-0faf-4d06-dc51-4d7204b528ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance : 0.94\n",
            "Accuracy with Manhattan distance : 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10.  : You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "â— Use PCA to reduce dimensionality\n",
        "â— Decide how many components to keep\n",
        "â— Use KNN for classification post-dimensionality reduction\n",
        "â— Evaluate the model\n",
        "â— Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data.DESCR\n",
        "\n",
        "\n",
        "\n",
        "  # ->  1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "# Problem:\n",
        "     Too many features (genes) relative to samples â†’ curse of dimensionality, risk of overfitting, and poor generalization.\n",
        "\n",
        "# Solution:\n",
        "   Apply Principal Component Analysis (PCA) after scaling the gene expression values.\n",
        "   PCA transforms the gene expression matrix into a new feature space where components capture maximum variance in descending order.\n",
        "    This way, instead of feeding all gene expressions directly, we work with only the most informative components, reducing noise and redundancy.\n",
        "\n",
        "# ðŸ”¹ 2. Decide How Many Components to Keep\n",
        "           Look at the explained variance ratio from PCA.\n",
        "           Plot a scree plot (variance vs. components) and identify the elbow point â€” where\n",
        "             additional components add diminishing returns.\n",
        "\n",
        "# In practice:\n",
        "Retain enough components to capture ~90â€“95% of total variance.\n",
        "If interpretability matters (e.g., linking back to biology), you may keep fewer components for easier analysis.\n",
        "\n",
        "# ðŸ”¹ 3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "# With PCA-transformed data:\n",
        "   Train a K-Nearest Neighbors (KNN) classifier, which works well after dimensionality reduction since PCA\n",
        "  removes irrelevant/noisy features.\n",
        "Try different distance metrics (Euclidean, Manhattan) and different values of k using cross-validation.\n",
        "\n",
        "# Benefits:\n",
        "PCA removes irrelevant gene expression noise.\n",
        "KNN is non-parametric, so it adapts well to the transformed feature space.\n",
        "\n",
        "# ðŸ”¹ 4. Evaluate the Model\n",
        "   Since biomedical datasets are usually small, use stratified k-fold cross-validation\n",
        "  (e.g., 5- or 10-fold) to get a reliable performance estimate.\n",
        "\n",
        "# Metrics:\n",
        "    Accuracy (overall performance).\n",
        "    Precision, Recall, F1-score (important in healthcare since false negatives/positives have different risks).\n",
        "    ROC-AUC (for multi-class cancer classification, use one-vs-rest ROC curves).\n",
        "\n",
        "# ðŸ”¹ 5. Justify the Pipeline to Stakeholders\n",
        "When explaining to clinicians, researchers, or decision-makers, you want to emphasize:\n",
        "# Dimensionality reduction with PCA:\n",
        "Prevents overfitting by removing irrelevant/noisy genes.\n",
        "Speeds up computation, making the model more scalable.\n",
        "\n",
        "# KNN classifier:\n",
        "Simple and interpretable â€” decisions are based on \"patients with similar gene expression profiles.\"\n",
        "No strong assumptions about data distribution (important in heterogeneous biomedical data).\n",
        "\n",
        "# Robust evaluation:\n",
        "Cross-validation ensures performance is generalizable and not just fitted to one dataset.\n",
        "Multiple metrics beyond accuracy (like recall for cancer detection) ensure clinical relevance.\n",
        "\n",
        "# Real-world readiness:\n",
        "\n",
        "Pipeline balances accuracy and interpretability.\n",
        "Scalable to new patient data once standardized preprocessing (scaling + PCA transformation) is established."
      ],
      "metadata": {
        "id": "Ps6VWygXrANG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}